{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51935b9a-2624-4b72-8ffe-43c52bc60726",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder  # Requires: pip install cross-encoder\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from python_scripts.user_article_tracking import (\n",
    "    initialize_graph_and_check_updates,\n",
    "    load_users_from_json,\n",
    "    get_update_message,\n",
    "    USER_PROFILE_DIR,\n",
    "KNOWLEDGE_GRAPH_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0899dbcc-2191-4e10-bba7-132a3d8a9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544df752-a5ec-485f-9dd3-031778703537",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration Parameters ---\n",
    "WIKI_FOLDER = \"wikipedia_articles_50_per_topic\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "# Use the Hugging Face identifier for your LLaMA model (adjust if needed)\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa4cb62-c66b-433b-a9a6-a45a0325b4c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU acceleration options for FAISS\n",
    "USE_GPU_FAISS = True    # Set to True if you have faiss-gpu installed\n",
    "FAISS_GPU_DEVICE = 0    # GPU device id for FAISS\n",
    "\n",
    "# Persistence paths for vector database components\n",
    "PERSISTENCE_DIR = Path(\"vector_database_v2\")\n",
    "PERSISTENCE_DIR.mkdir(exist_ok=True)\n",
    "VECTOR_PATH = PERSISTENCE_DIR / \"vectorizer.pkl\"\n",
    "TFIDF_PATH = PERSISTENCE_DIR / \"tfidf_matrix.npz\"\n",
    "FAISS_INDEX_PATH = PERSISTENCE_DIR / \"faiss.index\"\n",
    "TEXTS_PATH = PERSISTENCE_DIR / \"texts.pkl\"\n",
    "PATHS_PATH = PERSISTENCE_DIR / \"paths.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965e857f-5d93-48b8-ae45-98d2689ce3a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Load Articles ---\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"Load documents from text files and return a list of dictionaries with 'text' and 'path'.\"\"\"\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                path = os.path.join(root, file)\n",
    "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                    documents.append({\"text\": text, \"path\": path})\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca44c32c-b1c5-4a19-8abe-94cabd4813bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Build TF-IDF Index ---\n",
    "def build_sparse_index(texts):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "# --- Build FAISS Index ---\n",
    "def build_dense_index(embeddings, use_gpu=USE_GPU_FAISS):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            faiss_index = faiss.index_cpu_to_gpu(res, FAISS_GPU_DEVICE, index_cpu)\n",
    "        except AttributeError:\n",
    "            print(\"FAISS GPU resources not available. Falling back to CPU index.\")\n",
    "    index.add(embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8135f5-8d4a-4797-b3b5-91929a043a85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Hybrid Search ---\n",
    "def hybrid_search(query, vectorizer, tfidf_matrix, embedder, faiss_index, texts, paths, top_k=10):\n",
    "    \"\"\"Search for candidates and return a list of tuples: (index, text, file_path, combined_score).\"\"\"\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    sparse_scores = (tfidf_matrix @ query_tfidf.T).toarray().squeeze()\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    _, dense_indices = faiss_index.search(query_emb, top_k)\n",
    "    dense_scores = np.zeros(len(texts))\n",
    "    dense_scores[dense_indices[0]] = 1.0\n",
    "    combined_scores = sparse_scores + dense_scores\n",
    "    top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "    return [(i, texts[i], paths[i], combined_scores[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c960ae-cd91-4837-9696-275b7d7ae81f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Rerank Results ---\n",
    "def rerank(query, candidates, reranker_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Use the cross encoder to rerank the candidates.\n",
    "    Returns a list of tuples: (text, file_path, reranker_score)\n",
    "    \"\"\"\n",
    "    inputs = [(query, cand[1]) for cand in candidates]  # cand[1] is the document text\n",
    "    scores = reranker_model.predict(inputs)\n",
    "    reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return [(item[0][1], item[0][2], item[1]) for item in reranked]  # (text, path, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b49e21-9cbc-4e7c-9343-f6aea5ac2048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Generate Answer using Hugging Face Transformers ---\n",
    "def generate_answer(generator, query, contexts, max_context_tokens=300, num_contexts=2):\n",
    "    \"\"\"\n",
    "    Build a prompt using the top contexts and generate an answer.\n",
    "    contexts: list of tuples (text, file_path, score)\n",
    "    \"\"\"\n",
    "    truncated_contexts = []\n",
    "    for i, (text, path, score) in enumerate(contexts[:num_contexts]):\n",
    "        tokens = text.split()[:max_context_tokens]\n",
    "        truncated_text = \" \".join(tokens)\n",
    "        truncated_contexts.append((truncated_text, path))\n",
    "    \n",
    "    context_str = \"\\n\\n\".join([f\"[{i+1}] {ctx}\" for i, (ctx, _) in enumerate(truncated_contexts)])\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Use only the provided context to answer.\\n\\n\"\n",
    "        f\"Context:\\n{context_str}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\nAnswer:\"\n",
    "    )\n",
    "    output = generator(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    answer = generated_text[len(prompt):].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1be3885-0a58-47ec-be2d-e326f04d40ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Fallback Answer Function using Hugging Face Transformers ---\n",
    "def handle_fallback(generator, query):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI assistant. When you don't know something, be honest about it. \"\n",
    "        \"Provide clear, concise, and accurate responses.\"\n",
    "    )\n",
    "    prompt = f\"{system_prompt}\\n\\nUser: {query}\\nAssistant:\"\n",
    "    output = generator(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    full_response = generated_text[len(prompt):].strip()\n",
    "    if not full_response:\n",
    "        return \"I apologize, but I couldn't generate a response. Please try rephrasing your question.\"\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38f99a8-450e-4f4f-8c01-b7f38a05cab5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading models...\")\n",
    "# Load the SentenceTransformer and CrossEncoder with GPU support.\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL, device=\"cuda\")\n",
    "reranker_model = CrossEncoder(RERANKER_MODEL, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4b50e5b-06c7-4d59-96a3-5dee8306b843",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the LLaMA model from Hugging Face for text generation.\n",
    "print(\"Loading Llama model from Hugging Face...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "# Create a text-generation pipeline using the model and tokenizer.\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17df4648-4c40-4b70-bdd3-44223199eb99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading persisted vector database...\n",
      "FAISS GPU resources not available during index load. Using CPU index.\n"
     ]
    }
   ],
   "source": [
    "# Load or build the vector database.\n",
    "if VECTOR_PATH.exists() and TFIDF_PATH.exists() and FAISS_INDEX_PATH.exists() and TEXTS_PATH.exists() and PATHS_PATH.exists():\n",
    "    print(\"Loading persisted vector database...\")\n",
    "    with open(VECTOR_PATH, \"rb\") as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    tfidf_matrix = scipy.sparse.load_npz(TFIDF_PATH)\n",
    "    index_cpu = faiss.read_index(str(FAISS_INDEX_PATH))\n",
    "    if USE_GPU_FAISS:\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            faiss_index = faiss.index_cpu_to_gpu(res, FAISS_GPU_DEVICE, index_cpu)\n",
    "        except AttributeError:\n",
    "            print(\"FAISS GPU resources not available during index load. Using CPU index.\")\n",
    "            faiss_index = index_cpu\n",
    "    else:\n",
    "        faiss_index = index_cpu\n",
    "    with open(TEXTS_PATH, \"rb\") as f:\n",
    "        texts = pickle.load(f)\n",
    "    with open(PATHS_PATH, \"rb\") as f:\n",
    "        paths = pickle.load(f)\n",
    "else:\n",
    "    print(\"Building vector database from scratch...\")\n",
    "    docs = load_documents(WIKI_FOLDER)\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    paths = [doc[\"path\"] for doc in docs]\n",
    "    vectorizer, tfidf_matrix = build_sparse_index(texts)\n",
    "    embeddings = embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    faiss_index = build_dense_index(embeddings, use_gpu=USE_GPU_FAISS)\n",
    "    # Persist the vector database components.\n",
    "    with open(VECTOR_PATH, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    scipy.sparse.save_npz(TFIDF_PATH, tfidf_matrix)\n",
    "    if USE_GPU_FAISS:\n",
    "        try:\n",
    "            cpu_index = faiss.index_gpu_to_cpu(faiss_index)\n",
    "        except AttributeError:\n",
    "            print(\"index_gpu_to_cpu not available, saving the GPU index as is.\")\n",
    "            cpu_index = faiss_index\n",
    "        faiss.write_index(cpu_index, str(FAISS_INDEX_PATH))\n",
    "    else:\n",
    "        faiss.write_index(faiss_index, str(FAISS_INDEX_PATH))\n",
    "    with open(TEXTS_PATH, \"wb\") as f:\n",
    "        pickle.dump(texts, f)\n",
    "    with open(PATHS_PATH, \"wb\") as f:\n",
    "        pickle.dump(paths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de23f3-10c6-45de-a978-89b84bcf35bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your user ID (e.g., Emp1):  Emp3-mkt-ds\n",
      "\n",
      "Ask a question (or type 'exit'):  What is data science ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving candidates...\n",
      "\n",
      "Reranking results...\n",
      "\n",
      "Generating answer with Llama (via Transformers)...\n",
      "\n",
      "--- Answer ---\n",
      "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.\n",
      "\n",
      "--- Top Contributing Articles (file paths) ---\n",
      "[1] wikipedia_articles_50_per_topic/Data_science/Data science.txt\n",
      "[2] wikipedia_articles_50_per_topic/Data_science/Master in Data Science.txt\n",
      "[Init] Checking for article updates and building knowledge graph...\n",
      "\n",
      "--- User Article Updates ---\n",
      "No new article updates since your last interaction.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your user ID (e.g., Emp1):  Emp3-mkt-ds\n",
      "\n",
      "Ask a question (or type 'exit'):  tell me about marketing ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving candidates...\n",
      "\n",
      "Reranking results...\n",
      "\n",
      "Generating answer with Llama (via Transformers)...\n",
      "\n",
      "--- Answer ---\n",
      "Marketing is the act of satisfying and retaining customers through the creation, communication, delivery, and exchange of offerings that have value for customers, clients, partners, and society at large. It is a primary component of business management and commerce, and involves the use of various marketing mix elements, such as product, price, promotion, and place, to create a unique marketing strategy that meets the needs of the target market. Marketing can be conducted by the seller or by dedicated marketing firms, and may involve the use of market research and marketing orientations to inform marketing decisions.\n",
      "\n",
      "--- Top Contributing Articles (file paths) ---\n",
      "[1] wikipedia_articles_50_per_topic/Marketing/Marketing.txt\n",
      "[2] wikipedia_articles_50_per_topic/Sales/Marketing.txt\n",
      "[Init] Checking for article updates and building knowledge graph...\n",
      "\n",
      "--- User Article Updates ---\n",
      "Since your last visit, the following articles were updated: Sales/Marketing.txt (Sales)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your user ID (e.g., Emp1):  Emp2-sls\n",
      "\n",
      "Ask a question (or type 'exit'):  what is sales ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving candidates...\n",
      "\n",
      "Reranking results...\n",
      "\n",
      "Generating answer with Llama (via Transformers)...\n",
      "\n",
      "--- Answer ---\n",
      "Sales refers to the act of selling or offering goods or services in exchange for payment. It can also refer to the total amount of money earned from these transactions.\n",
      "\n",
      "--- Top Contributing Articles (file paths) ---\n",
      "[1] wikipedia_articles_50_per_topic/Sales/Sales promotion.txt\n",
      "[2] wikipedia_articles_50_per_topic/Sales/Sales tax.txt\n",
      "[Init] Checking for article updates and building knowledge graph...\n",
      "\n",
      "--- User Article Updates ---\n",
      "No new article updates since your last interaction.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your user ID (e.g., Emp1):  Emp2-sls\n",
      "\n",
      "Ask a question (or type 'exit'):  tell me more about sales ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving candidates...\n",
      "\n",
      "Reranking results...\n",
      "\n",
      "Generating answer with Llama (via Transformers)...\n",
      "\n",
      "--- Answer ---\n",
      "Sales is a crucial element of the promotional mix, which refers to the use of various marketing communications tactics to promote a product or service to a target audience. Sales promotions are designed to increase consumer demand, stimulate market demand, or improve product availability, and can be directed at either the customer, sales staff, or distribution channel members. Examples of sales promotion tactics include coupons, samples, premiums, point-of-purchase displays, contests, rebates, and sweepstakes. The primary goal of sales promotions is to attract new customers, hold present customers, counteract competition, and take advantage of opportunities revealed by market research.\n",
      "\n",
      "Sales is also a primary front-office division of major investment banks, responsible for trading activities. The sales component of sales and trading refers to the investment bank's sales force within this division, which is responsible for interacting directly with institutional clients to assess their needs, provide market commentary, and work with other members of the desk to price and execute trades. Sales members may also employ financial analysts to provide trading strategy advice to external and internal clients. The sales and trading function is critical to the operation of investment banks, as it helps to determine the direction of the firm's proprietary and flow positions, as well as the suggestions salespersons give to clients.\n",
      "\n",
      "--- Top Contributing Articles (file paths) ---\n",
      "[1] wikipedia_articles_50_per_topic/Sales/Sales promotion.txt\n",
      "[2] wikipedia_articles_50_per_topic/Sales/Sales and trading.txt\n",
      "[Init] Checking for article updates and building knowledge graph...\n",
      "\n",
      "--- User Article Updates ---\n",
      "Since your last visit, the following articles were updated: Sales/Sales presentation.txt (Sales)\n"
     ]
    }
   ],
   "source": [
    "users = load_users_from_json(USER_PROFILE_DIR)\n",
    "user_map = {u['id']: u for u in users}\n",
    "if os.path.exists(KNOWLEDGE_GRAPH_PATH):\n",
    "    pass\n",
    "else:\n",
    "    initialize_graph_and_check_updates(USER_PROFILE_DIR)\n",
    "while True:\n",
    "    user_id = input(\"\\nEnter your user ID (e.g., Emp1): \").strip()\n",
    "    if user_id not in user_map:\n",
    "        print(f\"User '{user_id}' not found. Try again.\")\n",
    "        continue\n",
    "    query = input(\"\\nAsk a question (or type 'exit'): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    print(\"\\nRetrieving candidates...\")\n",
    "    candidates = hybrid_search(query, vectorizer, tfidf_matrix, embedder, faiss_index, texts, paths)\n",
    "    \n",
    "    print(\"\\nReranking results...\")\n",
    "    reranked = rerank(query, candidates, reranker_model)\n",
    "    \n",
    "    if not reranked:\n",
    "        print(\"\\nUsing fallback model to generate response...\")\n",
    "        answer = handle_fallback(generator, query)\n",
    "        top_files = []\n",
    "    else:\n",
    "        print(\"\\nGenerating answer with Llama (via Transformers)...\")\n",
    "        answer = generate_answer(generator, query, reranked)\n",
    "        # Extract file paths of the top two contexts.\n",
    "        top_files = [item[1] for item in reranked[:2]]\n",
    "    \n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(answer)\n",
    "    \n",
    "    if top_files:\n",
    "        print(\"\\n--- Top Contributing Articles (file paths) ---\")\n",
    "        for idx, file_path in enumerate(top_files):\n",
    "            print(f\"[{idx+1}] {file_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo top contributing articles found.\")\n",
    "        \n",
    "    print(\"[Init] Checking for article updates and building knowledge graph...\")\n",
    "    update_summary = get_update_message(KNOWLEDGE_GRAPH_PATH, users)\n",
    "    print(\"\\n--- User Article Updates ---\")\n",
    "    print(update_summary.get(user_id, \"No user update info available.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e29ba-1183-4552-80a4-8e878dfc3986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
